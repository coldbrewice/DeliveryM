{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hwjh2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re  # 정규식을 위한 모듈\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import pyLDAvis\n",
    "from pyLDAvis import prepare  # pyLDAvis에서 prepare 직접 사용\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk 데이터 다운로드 (불용어 목록 다운로드)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = r\"C:\\Users\\hwjh2\\Desktop\\딜리버리엠\\인도 DB_표준화_210909.xlsx\"\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Product 열만 사용\n",
    "products = data['Product'].dropna().tolist()\n",
    "\n",
    "# 데이터 전처리: 텍스트 소문자화, 구두점 제거, 불용어 제거, 숫자 및 용량 제거\n",
    "def preprocess_text(text):\n",
    "    # 소문자화\n",
    "    text = text.lower()\n",
    "    # 숫자와 용량 제거 (예: \"100ml\", \"200g\", \"1kg\") + 숫자만 제거\n",
    "    text = re.sub(r'\\b\\d+\\s?(ml|g|kg|l|mg|oz|cm|mm|floz)?\\b', '', text)\n",
    "    # 구두점 제거\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    # 불용어 제거\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# 모든 텍스트에 대해 전처리 수행\n",
    "processed_products = [preprocess_text(product) for product in products]\n",
    "\n",
    "# 텍스트를 벡터화 (Bag of Words)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(processed_products)\n",
    "\n",
    "# K값을 시각화 결과에 따라 선택하고, 그 값을 사용하여 LDA 모델링\n",
    "num_topics_optimal = 10  # 엘보우 기법에서 얻은 최적 K값\n",
    "lda_optimal = LatentDirichletAllocation(n_components=num_topics_optimal, random_state=42)\n",
    "lda_optimal.fit(X)\n",
    "\n",
    "# pyLDAvis 시각화 준비\n",
    "# 어휘(vocab) 추출\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 문서-토픽 분포 추출\n",
    "doc_topic_dists = lda_optimal.transform(X)\n",
    "\n",
    "# 토픽-단어 분포 추출\n",
    "topic_term_dists = lda_optimal.components_\n",
    "\n",
    "# 단어 빈도 계산\n",
    "term_frequency = X.sum(axis=0).A1\n",
    "\n",
    "# 시각화 준비\n",
    "vis = pyLDAvis.prepare(\n",
    "    topic_term_dists=topic_term_dists, \n",
    "    doc_topic_dists=doc_topic_dists, \n",
    "    doc_lengths=X.sum(axis=1).A1, \n",
    "    vocab=vocab, \n",
    "    term_frequency=term_frequency\n",
    ")\n",
    "\n",
    "# 시각화 HTML로 저장\n",
    "pyLDAvis.save_html(vis, 'lda_visualization10.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
