{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hwjh2\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\hwjh2\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\hwjh2\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\hwjh2\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주요 단어 엑셀 파일이 저장되었습니다: C:\\Users\\hwjh2\\Desktop\\클러스터_주요_단어_combined.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# 데이터 로드 및 샘플링\n",
    "file_path = r\"C:\\Users\\hwjh2\\Desktop\\딜리버리엠\\인도 DB_표준화_210909.xlsx\"\n",
    "data = pd.read_excel(file_path)\n",
    "products = data['Product'].dropna().tolist()\n",
    "products_sampled = np.random.choice(products, size=int(len(products) * 0.05), replace=False)\n",
    "\n",
    "# 1. TF-IDF + KMeans 클러스터링\n",
    "def tfidf_clustering(texts, n_clusters=10):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    return vectorizer, labels\n",
    "\n",
    "# 2. Doc2Vec 임베딩 + KMeans 클러스터링\n",
    "def doc2vec_clustering(texts, n_clusters=10):\n",
    "    tagged_data = [TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(texts)]\n",
    "    model = Doc2Vec(vector_size=300, window=2, min_count=1, workers=4, epochs=100)\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    embeddings = [model.infer_vector(text.split()) for text in texts]\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "    return model, labels\n",
    "\n",
    "# 3. GloVe 임베딩 + KMeans 클러스터링\n",
    "def glove_clustering(texts, n_clusters=10):\n",
    "    glove_embeddings = {}\n",
    "    with open(r\"C:\\Users\\hwjh2\\Desktop\\딜리버리엠\\glove.6B\\glove.6B.300d.txt\", 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            glove_embeddings[word] = vector\n",
    "    \n",
    "    def get_glove_vector(text):\n",
    "        words = text.split()\n",
    "        word_vectors = [glove_embeddings.get(word, np.zeros(300)) for word in words]\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "\n",
    "    embeddings = [get_glove_vector(text) for text in texts]\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "    return glove_embeddings, labels\n",
    "\n",
    "# 4. Universal Sentence Encoder (USE) + KMeans 클러스터링\n",
    "def use_clustering(texts, n_clusters=10):\n",
    "    embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "    embeddings = embed(texts).numpy()  # 512차원 벡터\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "    return embed, embeddings, labels\n",
    "\n",
    "# 주요 단어 추출 함수 (TF-IDF)\n",
    "def get_top_keywords_tfidf(texts, vectorizer, labels, n_keywords=5):\n",
    "    top_keywords = []\n",
    "    for cluster_num in np.unique(labels):\n",
    "        cluster_indices = np.where(labels == cluster_num)[0]\n",
    "        cluster_texts = [texts[i] for i in cluster_indices]\n",
    "        X_cluster = vectorizer.transform(cluster_texts)\n",
    "        feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "        tfidf_scores = X_cluster.sum(axis=0).A1\n",
    "        sorted_idx = np.argsort(tfidf_scores)[::-1]\n",
    "        top_keywords.append(feature_names[sorted_idx][:n_keywords])\n",
    "    return top_keywords\n",
    "\n",
    "# 주요 단어 추출 함수 (Doc2Vec, GloVe)\n",
    "def get_top_keywords_combined(texts, labels, embeddings, n_keywords=5, embedding_model=None, glove_embeddings=None):\n",
    "    top_keywords = []\n",
    "    for cluster_num in np.unique(labels):\n",
    "        cluster_indices = np.where(labels == cluster_num)[0]\n",
    "        cluster_vectors = [embeddings[i] for i in cluster_indices]\n",
    "        cluster_center = np.mean(cluster_vectors, axis=0)\n",
    "\n",
    "        # Doc2Vec: 클러스터 중심에서 가장 가까운 단어를 찾기\n",
    "        if embedding_model is not None:\n",
    "            # 중심 벡터와 가장 유사한 단어들을 찾기\n",
    "            closest_words_doc2vec = embedding_model.dv.most_similar([cluster_center], topn=n_keywords)\n",
    "            doc2vec_keywords = [word for word, _ in closest_words_doc2vec]\n",
    "        else:\n",
    "            doc2vec_keywords = []\n",
    "\n",
    "        # GloVe: 클러스터 중심과 가장 가까운 단어를 찾기\n",
    "        if glove_embeddings is not None:\n",
    "            closest_words_glove = sorted(glove_embeddings.keys(), key=lambda x: np.linalg.norm(glove_embeddings[x] - cluster_center))[:n_keywords]\n",
    "        else:\n",
    "            closest_words_glove = []\n",
    "\n",
    "        # Doc2Vec와 GloVe 키워드를 합치기\n",
    "        top_keywords.append(list(set(doc2vec_keywords + closest_words_glove)))\n",
    "\n",
    "    return top_keywords\n",
    "\n",
    "\n",
    "# 주요 단어 추출 함수 (USE)\n",
    "def get_use_top_keywords(texts, labels, embeddings, n_keywords=5):\n",
    "    top_keywords = []\n",
    "    for cluster_num in np.unique(labels):\n",
    "        cluster_indices = np.where(labels == cluster_num)[0]\n",
    "        cluster_vectors = [embeddings[i] for i in cluster_indices]\n",
    "        cluster_center = np.mean(cluster_vectors, axis=0)\n",
    "        \n",
    "        # USE는 문장 중심으로 비교\n",
    "        closest_indices = np.argsort(np.linalg.norm(embeddings[cluster_indices] - cluster_center, axis=1))[:n_keywords]\n",
    "        top_keywords.append([texts[cluster_indices[i]] for i in closest_indices])\n",
    "    \n",
    "    return top_keywords\n",
    "\n",
    "# 클러스터링 및 주요 단어 추출\n",
    "vectorizer, tfidf_labels = tfidf_clustering(products_sampled, n_clusters=15)\n",
    "doc2vec_model, doc2vec_labels = doc2vec_clustering(products_sampled, n_clusters=15)\n",
    "glove_embeddings, glove_labels = glove_clustering(products_sampled, n_clusters=15)\n",
    "use_embed, use_embeddings, use_labels = use_clustering(products_sampled, n_clusters=15)\n",
    "\n",
    "tfidf_keywords = get_top_keywords_tfidf(products_sampled, vectorizer, tfidf_labels, n_keywords=5)\n",
    "doc2vec_keywords = get_top_keywords_combined(products_sampled, doc2vec_labels, \n",
    "                                            [doc2vec_model.infer_vector(text.split()) for text in products_sampled], \n",
    "                                            n_keywords=5, embedding_model=doc2vec_model, glove_embeddings=None)\n",
    "glove_keywords = get_top_keywords_combined(products_sampled, glove_labels, \n",
    "                                          [np.mean([glove_embeddings.get(word, np.zeros(300)) for word in text.split()], axis=0) \n",
    "                                           for text in products_sampled], n_keywords=5, glove_embeddings=glove_embeddings)\n",
    "use_keywords = get_use_top_keywords(products_sampled, use_labels, use_embeddings, n_keywords=5)\n",
    "\n",
    "# 결과 DataFrame 생성\n",
    "keywords_df = pd.DataFrame({\n",
    "    'Cluster': [f'Cluster {i+1}' for i in range(15)],\n",
    "    'TF-IDF': [' '.join(tfidf_keywords[i]) for i in range(15)],\n",
    "    'Doc2Vec': [' '.join(doc2vec_keywords[i]) for i in range(15)],\n",
    "    'GloVe': [' '.join(glove_keywords[i]) for i in range(15)],\n",
    "    'USE': [' '.join(use_keywords[i]) for i in range(15)]\n",
    "})\n",
    "\n",
    "# 제품과 클러스터 정보를 포함하는 DataFrame 생성\n",
    "df = pd.DataFrame({\n",
    "    'Product': products_sampled,\n",
    "    'TF-IDF Cluster': tfidf_labels,\n",
    "    'Doc2Vec Cluster': doc2vec_labels,\n",
    "    'GloVe Cluster': glove_labels\n",
    "})\n",
    "\n",
    "# 'Cluster' 열을 int 형식으로 변환\n",
    "keywords_df['Cluster'] = keywords_df['Cluster'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# 'TF-IDF Cluster' 열도 int 형식으로 변환\n",
    "df['TF-IDF Cluster'] = df['TF-IDF Cluster'].astype(int)\n",
    "\n",
    "# 주요 키워드와 함께 결과 결합\n",
    "df_keywords = pd.merge(df, keywords_df, left_on='TF-IDF Cluster', right_on='Cluster', how='left')\n",
    "\n",
    "# 최종 Excel 파일 저장\n",
    "output_file_path = r\"C:\\Users\\hwjh2\\Desktop\\클러스터_주요_단어_combined.xlsx\"\n",
    "df_keywords.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"주요 단어 엑셀 파일이 저장되었습니다: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hwjh2\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\hwjh2\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\hwjh2\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\hwjh2\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel 파일이 성공적으로 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import tensorflow_hub as hub\n",
    "import gensim  # gensim 라이브러리 임포트 추가\n",
    "\n",
    "# 데이터 로드 및 샘플링\n",
    "file_path = r\"C:\\Users\\hwjh2\\Desktop\\딜리버리엠\\인도 DB_표준화_210909.xlsx\"\n",
    "data = pd.read_excel(file_path)\n",
    "products = data['Product'].dropna().tolist()\n",
    "products_sampled = np.random.choice(products, size=int(len(products) * 0.1), replace=False)\n",
    "\n",
    "# 1. TF-IDF + KMeans 클러스터링\n",
    "def tfidf_clustering(texts, n_clusters=10):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    return vectorizer, labels\n",
    "\n",
    "# 2. Doc2Vec 임베딩 + KMeans 클러스터링\n",
    "def doc2vec_clustering(texts, n_clusters=10):\n",
    "    tagged_data = [TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(texts)]\n",
    "    model = Doc2Vec(vector_size=300, window=2, min_count=1, workers=4, epochs=100)\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    embeddings = [model.infer_vector(text.split()) for text in texts]\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "    return model, labels\n",
    "\n",
    "# 3. GloVe 임베딩 + KMeans 클러스터링\n",
    "def glove_clustering(texts, n_clusters=10):\n",
    "    glove_embeddings = {}\n",
    "    with open(r\"C:\\Users\\hwjh2\\Desktop\\딜리버리엠\\glove.6B\\glove.6B.300d.txt\", 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            glove_embeddings[word] = vector\n",
    "    \n",
    "    def get_glove_vector(text):\n",
    "        words = text.split()\n",
    "        word_vectors = [glove_embeddings.get(word, np.zeros(300)) for word in words]\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "\n",
    "    embeddings = [get_glove_vector(text) for text in texts]\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "    return glove_embeddings, labels\n",
    "\n",
    "# 4. Universal Sentence Encoder (USE) + KMeans 클러스터링\n",
    "def use_clustering(texts, n_clusters=10):\n",
    "    embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "    embeddings = embed(texts).numpy()  # 512차원 벡터\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "    return embed, embeddings, labels\n",
    "\n",
    "# 주요 단어 추출 함수 (TF-IDF)\n",
    "def get_top_keywords_tfidf(texts, vectorizer, labels, n_keywords=5):\n",
    "    top_keywords = []\n",
    "    for cluster_num in np.unique(labels):\n",
    "        cluster_indices = np.where(labels == cluster_num)[0]\n",
    "        cluster_texts = [texts[i] for i in cluster_indices]\n",
    "        X_cluster = vectorizer.transform(cluster_texts)\n",
    "        feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "        tfidf_scores = X_cluster.sum(axis=0).A1\n",
    "        sorted_idx = np.argsort(tfidf_scores)[::-1]\n",
    "        top_keywords.append(feature_names[sorted_idx][:n_keywords])\n",
    "    return top_keywords\n",
    "\n",
    "# 주요 단어 추출 함수 (Doc2Vec, GloVe)\n",
    "def get_top_keywords_generic(texts, labels, embeddings, n_keywords=5, embedding_model=None):\n",
    "    top_keywords = []\n",
    "    for cluster_num in np.unique(labels):\n",
    "        cluster_indices = np.where(labels == cluster_num)[0]\n",
    "        cluster_texts = [texts[i] for i in cluster_indices]\n",
    "        cluster_vectors = [embeddings[i] for i in cluster_indices]\n",
    "        cluster_center = np.mean(cluster_vectors, axis=0)\n",
    "\n",
    "        # Doc2Vec인 경우: 가장 가까운 단어를 찾는 과정\n",
    "        if embedding_model is not None:\n",
    "            if isinstance(embedding_model, gensim.models.KeyedVectors):  # GloVe와 같은 모델\n",
    "                closest_words = sorted(embedding_model.keys(), key=lambda x: np.linalg.norm(embedding_model[x] - cluster_center))[:n_keywords]\n",
    "                top_keywords.append(closest_words)\n",
    "            else:  # Doc2Vec 모델\n",
    "                closest_words = []\n",
    "                for word in embedding_model.wv.index_to_key:  # Doc2Vec의 경우\n",
    "                    word_vector = embedding_model.wv[word]\n",
    "                    similarity = np.dot(cluster_center, word_vector) / (np.linalg.norm(cluster_center) * np.linalg.norm(word_vector))\n",
    "                    closest_words.append((word, similarity))\n",
    "                \n",
    "                # 유사도 기준으로 정렬하여 가장 유사한 단어 5개를 선택\n",
    "                closest_words = sorted(closest_words, key=lambda x: x[1], reverse=True)[:n_keywords]\n",
    "                top_keywords.append([word for word, _ in closest_words])\n",
    "        else:\n",
    "            # 임베딩이 없으면\n",
    "            top_keywords.append([])\n",
    "\n",
    "    return top_keywords\n",
    "\n",
    "# 주요 단어 추출 함수 (USE)\n",
    "def get_use_top_keywords(texts, labels, embeddings, n_keywords=5):\n",
    "    top_keywords = []\n",
    "    for cluster_num in np.unique(labels):\n",
    "        cluster_indices = np.where(labels == cluster_num)[0]\n",
    "        cluster_vectors = [embeddings[i] for i in cluster_indices]\n",
    "        cluster_center = np.mean(cluster_vectors, axis=0)\n",
    "        \n",
    "        # USE는 문장 중심으로 비교\n",
    "        closest_indices = np.argsort(np.linalg.norm(embeddings[cluster_indices] - cluster_center, axis=1))[:n_keywords]\n",
    "        top_keywords.append([texts[cluster_indices[i]] for i in closest_indices])\n",
    "    \n",
    "    return top_keywords\n",
    "\n",
    "# 클러스터링 및 주요 단어 추출\n",
    "vectorizer, tfidf_labels = tfidf_clustering(products_sampled, n_clusters=15)\n",
    "doc2vec_model, doc2vec_labels = doc2vec_clustering(products_sampled, n_clusters=15)\n",
    "glove_embeddings, glove_labels = glove_clustering(products_sampled, n_clusters=15)\n",
    "use_embed, use_embeddings, use_labels = use_clustering(products_sampled, n_clusters=15)\n",
    "\n",
    "tfidf_keywords = get_top_keywords_tfidf(products_sampled, vectorizer, tfidf_labels, n_keywords=5)\n",
    "doc2vec_keywords = get_top_keywords_generic(products_sampled, doc2vec_labels, \n",
    "                                            [doc2vec_model.infer_vector(text.split()) for text in products_sampled], \n",
    "                                            n_keywords=5, embedding_model=doc2vec_model)\n",
    "glove_keywords = get_top_keywords_generic(products_sampled, glove_labels, \n",
    "                                          [np.mean([glove_embeddings.get(word, np.zeros(300)) for word in text.split()], axis=0) \n",
    "                                           for text in products_sampled], n_keywords=5)\n",
    "use_keywords = get_use_top_keywords(products_sampled, use_labels, use_embeddings, n_keywords=5)\n",
    "\n",
    "# 결과 DataFrame 생성\n",
    "keywords_df = pd.DataFrame({\n",
    "    'Cluster': [f'Cluster {i+1}' for i in range(15)],\n",
    "    'TF-IDF': [' '.join(tfidf_keywords[i]) for i in range(15)],\n",
    "    'Doc2Vec': [' '.join(doc2vec_keywords[i]) for i in range(15)],\n",
    "    'GloVe': [' '.join(glove_keywords[i]) for i in range(15)],\n",
    "    'USE': [' '.join(use_keywords[i]) for i in range(15)]\n",
    "})\n",
    "\n",
    "# 제품과 클러스터 정보를 포함하는 DataFrame 생성\n",
    "df = pd.DataFrame({\n",
    "    'Product': products_sampled,\n",
    "    'TF-IDF Cluster': tfidf_labels,\n",
    "    'Doc2Vec Cluster': doc2vec_labels,\n",
    "    'GloVe Cluster': glove_labels\n",
    "})\n",
    "\n",
    "# 'Cluster' 열을 int 형식으로 변환\n",
    "keywords_df['Cluster'] = keywords_df['Cluster'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# 'TF-IDF Cluster' 열도 int 형식으로 변환\n",
    "df['TF-IDF Cluster'] = df['TF-IDF Cluster'].astype(int)\n",
    "\n",
    "# 주요 키워드와 함께 결과 결합\n",
    "df_keywords = pd.merge(df, keywords_df, left_on='TF-IDF Cluster', right_on='Cluster', how='left')\n",
    "\n",
    "# 최종 Excel 파일 저장\n",
    "output_file_path = r\"C:\\Users\\hwjh2\\Desktop\\클러스터_주요_단어doc.xlsx\"\n",
    "df_keywords.to_excel(output_file_path, index=False)\n",
    "\n",
    "# 완료 메시지 출력\n",
    "print(\"Excel 파일이 성공적으로 생성되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
